{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNC:\n",
    "    def __init__(self,input_size,output_size,seq_len,num_words,word_size,read_heads):\n",
    "        with tf.device('/device:GPU:1'):\n",
    "            self.input_size = input_size\n",
    "            self.output_size = output_size\n",
    "\n",
    "            self.num_words = num_words\n",
    "            self.word_size = word_size\n",
    "\n",
    "            self.read_heads = read_heads\n",
    "\n",
    "            self.interface_size = (word_size*read_heads) + (3*word_size)+ (5*read_heads)+3\n",
    "\n",
    "            self.controller_input_size = (read_heads*word_size)+input_size\n",
    "\n",
    "            self.controller_output_size = output_size + self.interface_size\n",
    "\n",
    "            self.output_vector = tf.truncated_normal([1,self.output_size],stddev=0.1)\n",
    "            self.interface_vector = tf.truncated_normal([1,self.interface_size],stddev=0.1)\n",
    "\n",
    "            self.memory_matrix = tf.zeros([num_words,word_size])\n",
    "\n",
    "            self.usage_vector = tf.fill([num_words,1],1e-6)\n",
    "            self.temp_link_matrix = tf.zeros([num_words,num_words])\n",
    "\n",
    "            self.precedence_weighting  = tf.zeros([num_words,1])\n",
    "\n",
    "            self.read_weightings = tf.fill([num_words,read_heads],1e-6)\n",
    "            self.write_weightings = tf.fill([num_words,1],1e-6)\n",
    "            self.read_vectors = tf.fill([read_heads,word_size],1e-6)\n",
    "\n",
    "            # Controller\n",
    "            self.input_x = tf.placeholder(tf.float32,shape=[seq_len*2,self.input_size],name = \"input_x\")\n",
    "            self.output_y = tf.placeholder(tf.float32,shape=[seq_len*2,self.output_size],name=\"output_y\")\n",
    "\n",
    "\n",
    "            self.weights1 = tf.get_variable(\"weights1\",shape=[self.controller_input_size,32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.biases1 = tf.get_variable(\"biases1\",shape=[32],initializer=tf.zeros_initializer())\n",
    "            self.weights2 = tf.get_variable(\"weights2\",shape=[32,self.controller_output_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.biases2 = tf.get_variable(\"biases2\",shape=[self.controller_output_size])\n",
    "\n",
    "            self.output_vector_weights = tf.get_variable(\"Wy\",shape=[self.controller_output_size,self.output_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            self.interface_weights = tf.get_variable(\"Wiv\",shape=[self.controller_output_size,self.interface_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "            self.read_vectors_weights = tf.get_variable(\"Wr\",shape=[self.read_heads*self.word_size,self.output_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "    def content_lookup(self,key,key_strength):\n",
    "        normalized_memory = tf.nn.l2_normalize(self.memory_matrix,1)\n",
    "        normalized_key = tf.nn.l2_normalize(key,0)\n",
    "        \n",
    "        z = tf.matmul(normalized_memory,normalized_key,transpose_b=True)\n",
    "        \n",
    "        return tf.nn.softmax(key_strength*z,0)\n",
    "    \n",
    "    # used to provided new locations for writing\n",
    "    def calc_allocation_weighting(self):\n",
    "        # multiply usage vector by -1 to get locations in ascending order of usage\n",
    "        sorted_usage_vector,free_list = tf.nn.top_k(-1*self.usage_vector,k = self.num_words)\n",
    "        # since usage vector was multiplied by -1,after sorted, return to its original value\n",
    "        sorted_usage_vector = sorted_usage_vector*-1\n",
    "        \n",
    "        cumulative_product = tf.cumprod(sorted_usage_vector,axis=0,exclusive=True)\n",
    "        unordered_allocation_weighting =  (1-sorted_usage_vector)*cumulative_product\n",
    "        \n",
    "        allocation_weights = tf.zeros([self.num_words])\n",
    "        identity_matrix = tf.constant(np.identity(self.num_words,dtype=np.float32))\n",
    "        \n",
    "        for pos, idx in enumerate(tf.unstack(free_list[0])):\n",
    "            #flatten\n",
    "            m = tf.squeeze(tf.slice(identity_matrix, [idx, 0], [1, -1]))\n",
    "            #add to weight matrix\n",
    "            allocation_weights += m*unordered_allocation_weighting[0, pos]\n",
    "        #the allocation weighting for each row in memory\n",
    "        return tf.reshape(allocation_weights, [self.num_words, 1])\n",
    "\n",
    "    \n",
    "    def one_plus(self,x):\n",
    "        return 1+tf.nn.softplus(tf.expand_dims(x,0))\n",
    "    \n",
    "    def time_step(self,x):\n",
    "        #print(x)\n",
    "        step_input = tf.concat([x,tf.reshape(self.read_vectors,[1,self.read_heads*self.word_size])],1)\n",
    "        #print(step_input)\n",
    "        #controller forward propagation\n",
    "        layer1_activation = tf.nn.relu(tf.matmul(step_input,self.weights1)+self.biases1)\n",
    "        #print(\"layer 1 act\",layer1_activation)\n",
    "        layer2_activation = tf.nn.relu(tf.matmul(layer1_activation,self.weights2)+self.biases2)\n",
    "        #print(\"layer 2 act\",layer2_activation)\n",
    "        \n",
    "        #output vector\n",
    "        self.output_vector = tf.matmul(layer2_activation,self.output_vector_weights)\n",
    "        #print(\"output vector\",self.output_vector)\n",
    "        \n",
    "        #interface vector\n",
    "        self.interface_vector = tf.matmul(layer2_activation,self.interface_weights)\n",
    "        #print(\"Interface vector\",self.interface_vector)\n",
    "        \n",
    "        #Interact with the memory(read and write)\n",
    "        ##Slice interface vector to get the 10 components of it, the partition its an indexes vector(values from 0 to 9)\n",
    "        partition_indexes = tf.constant([[0]*(self.read_heads*self.word_size) #read keys\n",
    "                                +[1]*(self.read_heads)#read strengths \n",
    "                                +[2]*(self.word_size)\n",
    "                                +[3] #write strength\n",
    "                                +[4]*(self.word_size) #erase vector\n",
    "                                +[5]*(self.word_size) #write vector\n",
    "                                +[6]*(self.read_heads) #free gates\n",
    "                                +[7] #allocation gate\n",
    "                                +[8] #write gate\n",
    "                                +[9]*(self.read_heads*3) #read modes\n",
    "                                \n",
    "                                ],dtype = tf.int32)\n",
    "        #print(\"indexes\",partition_indexes)\n",
    "        \n",
    "        #print(partition_indexes)\n",
    "        (read_keys,read_strengths,write_key\n",
    "        ,write_strength,erase_vector,write_vector,\n",
    "        free_gates,allocation_gate,write_gate,read_modes) = tf.dynamic_partition(self.interface_vector,partition_indexes,10)\n",
    "        \n",
    "        ##Make every value have the correct shape and be in the correct domain\n",
    "        read_keys = tf.reshape(read_keys,[self.read_heads,self.word_size])\n",
    "        #print(\"read keys\",read_keys)\n",
    "        \n",
    "        read_strengths = self.one_plus(read_strengths)\n",
    "        #print(\"read_strengts\",read_strengths)\n",
    "        \n",
    "        write_key = tf.expand_dims(write_key,0)\n",
    "        #print(\"write key\",write_key)\n",
    "        write_strength = self.one_plus(write_strength)\n",
    "        #print(\"write strength\",write_strength)\n",
    "        \n",
    "        erase_vector = tf.nn.sigmoid(tf.expand_dims(erase_vector,0))\n",
    "        write_vector = tf.expand_dims(write_vector,0)\n",
    "        \n",
    "        free_gates =  tf.nn.sigmoid(tf.expand_dims(free_gates,0))\n",
    "        allocation_gate = tf.nn.sigmoid(allocation_gate)\n",
    "        write_gate = tf.nn.sigmoid(write_gate)\n",
    "        \n",
    "        read_modes = tf.nn.softmax(tf.reshape(read_modes,[3,self.read_heads]))\n",
    "        \n",
    "        \n",
    "        ## Writing to memory(dynamic allocation and content lookup)\n",
    "        ### dynamic memory allocation\n",
    "        retention_vector = tf.reduce_prod(1-free_gates*self.read_weightings,reduction_indices=1)\n",
    "        \n",
    "        self.usage_vector = (self.usage_vector + self.write_weightings  - self.usage_vector* self.write_weightings)*retention_vector\n",
    "        \n",
    "        allocation_weights = self.calc_allocation_weighting()\n",
    "        \n",
    "        ### content lookup for  writing\n",
    "        write_content_weigths = self.content_lookup(write_key,write_strength)\n",
    "        \n",
    "        ### final write weights\n",
    "        self.write_weightings = write_gate*(allocation_gate*allocation_weights+(1-allocation_gate)*write_content_weigths)\n",
    "        #print(self.write_weightings )\n",
    "        \n",
    "        ### final writing to memory(first erase, then write)\n",
    "        self.memory_matrix  = self.memory_matrix * (1-tf.matmul(self.write_weightings,erase_vector))+(tf.matmul(self.write_weightings,write_vector))\n",
    "        \n",
    "        ## reading from memory(by content and by temporal order)\n",
    "        \n",
    "        ### temporal order\n",
    "        #### temporal link matrix update using write weights, and previus precedence weighitngs\n",
    "        #print(\"antes weightis\",self.write_weightings)\n",
    "        write_weightsi = tf.matmul(self.write_weightings,tf.ones([1,self.num_words]))\n",
    "        #print(\"yua\")\n",
    "        #print(self.precedence_weighting)\n",
    "        self.temp_link_matrix = (1-write_weightsi-tf.transpose(write_weightsi)) * self.temp_link_matrix + tf.matmul(self.write_weightings,self.precedence_weighting,transpose_b=True)\n",
    "        self.temp_link_matrix = self.temp_link_matrix * (tf.ones([self.num_words,self.num_words]) - tf.constant(np.identity(self.num_words,dtype=np.float32)))\n",
    "        \n",
    "        \n",
    "        self.precedence_weighting = (1 - tf.reduce_sum(self.write_weightings,reduction_indices=0))* self.precedence_weighting + self.write_weightings\n",
    "        ### read modes (backguard,content,forward)\n",
    "        back_weigthing = read_modes[0]*tf.matmul(self.temp_link_matrix,self.read_weightings,transpose_a=True)\n",
    "        #print(back_weigthing)\n",
    "        content_weigthing = read_modes[1]*self.content_lookup(read_keys,read_strengths)\n",
    "        #print(content_weigthing)\n",
    "        forward_weithing = read_modes[2]*tf.matmul(self.temp_link_matrix,self.read_weightings)\n",
    "        #print(forward_weithing)\n",
    "        \n",
    "        self.read_weightings  = back_weigthing + content_weigthing + forward_weithing\n",
    "        \n",
    "        self.read_vectors = tf.transpose(tf.matmul(self.memory_matrix,self.read_weightings,transpose_a=True))\n",
    "        #print(self.memory_matrix)\n",
    "        #print(self.read_weightings)\n",
    "        #print(self.read_vectors)\n",
    "        \n",
    "        ### apply weights to read vectors\n",
    "        weighted_read_vectors = tf.matmul(tf.reshape(self.read_vectors,[1,self.read_heads*self.word_size]),self.read_vectors_weights)\n",
    "        #print(self.output_vector)\n",
    "        #print(weighted_read_vectors)\n",
    "        return self.output_vector + weighted_read_vectors\n",
    "    \n",
    "    #output list of numbers (one hot encoded) by running the step function\n",
    "    def run(self):\n",
    "        big_out = []\n",
    "        for t, seq in enumerate(tf.unstack(self.input_x, axis=0)):\n",
    "            seq = tf.expand_dims(seq, 0)\n",
    "            y = self.time_step(seq)\n",
    "            print(y)\n",
    "            big_out.append(y)\n",
    "        return tf.stack(big_out, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "\n",
    "    #generate the input output sequences, randomly intialized\n",
    "    num_seq = 10\n",
    "    seq_len = 6\n",
    "    seq_width = 4\n",
    "    iterations = 1000\n",
    "    con = np.random.randint(0, seq_width,size=seq_len)\n",
    "    seq = np.zeros((seq_len, seq_width))\n",
    "    seq[np.arange(seq_len), con] = 1\n",
    "    end = np.asarray([[-1]*seq_width])\n",
    "    zer = np.zeros((seq_len, seq_width))\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        #training time\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            #init the DNC\n",
    "            dnc = DNC(input_size=seq_width, output_size=seq_width, seq_len=seq_len, num_words=10, word_size=10, read_heads=1)\n",
    "            \n",
    "            #calculate the predicted output\n",
    "            output_logits = tf.squeeze(dnc.run())\n",
    "            output = tf.nn.sigmoid(output_logits)\n",
    "            #print(output,dnc.output_y)\n",
    "            #compare prediction to reality, get loss via sigmoid cross entropy\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_logits, labels=dnc.output_y))\n",
    "            #print(loss)\n",
    "            #use regularizers for each layer of the controller\n",
    "            regularizers = (tf.nn.l2_loss(dnc.weights1) + tf.nn.l2_loss(dnc.weights2) +\n",
    "                            tf.nn.l2_loss(dnc.biases1) + tf.nn.l2_loss(dnc.biases2))\n",
    "            #to help the loss convergence faster\n",
    "            loss += 5e-4 * regularizers\n",
    "            #optimize the entire thing (memory + controller) using gradient descent. dope\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "            \n",
    "            #initialize input output pairs\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            final_i_data = np.concatenate((seq, zer), axis=0)\n",
    "            final_o_data = np.concatenate((zer, seq), axis=0)\n",
    "            #for each iteration\n",
    "            for i in range(0, iterations+1):\n",
    "                #feed in each input output pair\n",
    "                \n",
    "                feed_dict = {dnc.input_x : final_i_data, dnc.output_y: final_o_data}\n",
    "                #make predictions\n",
    "                \n",
    "                l, _, predictions,write_weights = sess.run([loss, optimizer, output,dnc.write_weightings], feed_dict=feed_dict)\n",
    "                #print(write_weights)\n",
    "                if i%10==0:\n",
    "                    #print(read_weights)\n",
    "                    print(i,l)\n",
    "            #print results\n",
    "            print(final_i_data)\n",
    "            print(final_o_data)\n",
    "            print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_21:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_43:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_65:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_87:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_109:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_131:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_153:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_175:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_197:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_219:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_241:0\", shape=(1, 4), dtype=float32)\n",
      "Tensor(\"add_263:0\", shape=(1, 4), dtype=float32)\n",
      "0 0.695148\n",
      "10 0.660034\n",
      "20 0.62867\n",
      "30 0.600074\n",
      "40 0.579267\n",
      "50 0.564364\n",
      "60 0.55237\n",
      "70 0.54265\n",
      "80 0.534554\n",
      "90 0.527734\n",
      "100 0.521704\n",
      "110 0.516343\n",
      "120 0.511939\n",
      "130 0.507892\n",
      "140 0.50407\n",
      "150 0.500261\n",
      "160 0.496481\n",
      "170 0.49275\n",
      "180 0.489082\n",
      "190 0.485491\n",
      "200 0.481918\n",
      "210 0.478427\n",
      "220 0.474891\n",
      "230 0.471187\n",
      "240 0.467381\n",
      "250 0.463491\n",
      "260 0.459587\n",
      "270 0.455655\n",
      "280 0.451699\n",
      "290 0.447714\n",
      "300 0.443688\n",
      "310 0.439557\n",
      "320 0.435277\n",
      "330 0.430829\n",
      "340 0.426087\n",
      "350 0.421286\n",
      "360 0.416356\n",
      "370 0.411169\n",
      "380 0.406082\n",
      "390 0.400655\n",
      "400 0.395088\n",
      "410 0.389591\n",
      "420 0.384067\n",
      "430 0.378491\n",
      "440 0.37281\n",
      "450 0.36711\n",
      "460 0.361566\n",
      "470 0.355994\n",
      "480 0.350422\n",
      "490 0.34466\n",
      "500 0.338618\n",
      "510 0.33238\n",
      "520 0.325987\n",
      "530 0.319289\n",
      "540 0.312145\n",
      "550 0.304515\n",
      "560 0.295618\n",
      "570 0.286863\n",
      "580 0.277511\n",
      "590 0.268232\n",
      "600 0.25872\n",
      "610 0.249244\n",
      "620 0.239741\n",
      "630 0.230013\n",
      "640 0.220207\n",
      "650 0.209382\n",
      "660 0.19762\n",
      "670 0.185262\n",
      "680 0.171144\n",
      "690 0.154977\n",
      "700 0.135721\n",
      "710 0.181075\n",
      "720 0.147905\n",
      "730 0.130884\n",
      "740 0.126805\n",
      "750 0.121014\n",
      "760 0.115751\n",
      "770 0.110181\n",
      "780 0.104465\n",
      "790 0.0988863\n",
      "800 0.0936041\n",
      "810 0.0887696\n",
      "820 0.0840196\n",
      "830 0.0794866\n",
      "840 0.0751745\n",
      "850 0.0710283\n",
      "860 0.0673942\n",
      "870 0.450695\n",
      "880 0.214882\n",
      "890 0.12687\n",
      "900 0.0976092\n",
      "910 0.0830735\n",
      "920 0.0822147\n",
      "930 0.0787493\n",
      "940 0.0764094\n",
      "950 0.0741808\n",
      "960 0.0722522\n",
      "970 0.0703869\n",
      "980 0.0686053\n",
      "990 0.0668125\n",
      "1000 0.0650735\n",
      "[[ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "[[  1.19183697e-01   7.70616531e-02   1.15297034e-01   1.32536560e-01]\n",
      " [  2.22821664e-02   3.41605209e-02   1.49147892e-02   1.58721078e-02]\n",
      " [  6.82571298e-03   1.41016953e-02   1.62593520e-03   4.89091221e-03]\n",
      " [  1.79680996e-03   1.32360058e-02   2.81430781e-04   1.33269012e-03]\n",
      " [  1.14513480e-03   1.71284117e-02   2.93726807e-05   9.11184005e-04]\n",
      " [  1.38730055e-03   7.30285794e-02   2.06432760e-05   5.44837816e-03]\n",
      " [  2.01425999e-02   5.68112016e-01   1.43049387e-04   2.00765476e-01]\n",
      " [  1.46187723e-01   2.36138508e-01   1.98789546e-03   8.57506931e-01]\n",
      " [  8.02331507e-01   6.65516427e-06   8.44864100e-02   2.04471312e-02]\n",
      " [  2.74551157e-02   9.38703803e-31   9.72601831e-01   2.66886779e-09]\n",
      " [  9.99848366e-01   0.00000000e+00   2.81202448e-12   1.36441092e-14]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
