{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOGDIR = \"unit_logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_writer = tf.summary.FileWriter(LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNC:\n",
    "    def __init__(self,input_size,output_size,seq_len,num_words,word_size,read_heads,device = '/device:GPU:1'):\n",
    "        self.output_size = output_size\n",
    "        self.controller_output_size = output_size\n",
    "        self.read_heads = read_heads\n",
    "        self.word_size = word_size\n",
    "        self.num_words = num_words\n",
    "        \n",
    "        with tf.device(device):\n",
    "            #controller\n",
    "            \n",
    "            self.read_weightings = tf.Variable( tf.fill([num_words,read_heads],1e-8,name = \"read_weights_wr_init\"),trainable=False,name=\"read_weights_wr\")\n",
    "            self.read_vectors = tf.Variable(tf.fill([read_heads,word_size],1e-8,name=\"init_read_vectors\"),trainable=False,name=\"read_vectors_r\")\n",
    "            \n",
    "            self.write_weightings = tf.Variable( tf.fill([num_words,1],1e-8,name = \"write_weights_wr_init\"),trainable=False,name=\"write_weights_wr\")\n",
    "            self.write_vector = tf.Variable(tf.fill([1,word_size],1e-8,name=\"init_write_vectors\"),trainable=False,name=\"write_vector_v\")\n",
    "            \n",
    "            self.memory_matrix = tf.Variable( tf.truncated_normal([num_words,word_size]),trainable=False,name = \"memory_matrix_M\")\n",
    "            \n",
    "            with tf.name_scope(\"calc_output_vector_y/\"):\n",
    "                self.read_vectors_weights = tf.get_variable(\"Wr\",shape=[self.read_heads*self.word_size,self.output_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            with tf.name_scope(\"train_data/inputs_x\"):        \n",
    "                \n",
    "                \n",
    "                self.input_x = tf.placeholder(tf.float32,shape=[1,input_size],name = \"input_x\")\n",
    "                \n",
    "            with tf.name_scope(\"train_data/outputs_y\"):    \n",
    "                self.output_y = tf.placeholder(tf.float32,shape=[1,output_size],name=\"output_y\")\n",
    "            \n",
    "            \n",
    "\n",
    "            with tf.variable_scope(\"feed_forward/layer1\"):\n",
    "                self.weights1 = tf.get_variable(\"weights1\",shape=[input_size,32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "                self.biases1 = tf.get_variable(\"biases1\",shape=[32],initializer=tf.zeros_initializer())\n",
    "                layer1_activation = tf.nn.relu(tf.matmul(self.input_x,self.weights1)+self.biases1)\n",
    "            with tf.variable_scope(\"feed_forward/layer2\"):\n",
    "                self.weights2 = tf.get_variable(\"weights2\",shape=[32,output_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "                self.biases2 = tf.get_variable(\"biases2\",shape=[output_size])\n",
    "                layer2_activation = tf.nn.relu(tf.matmul(layer1_activation,self.weights2)+self.biases2)\n",
    "                self.output_logits = layer2_activation\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            with tf.name_scope(\"calc_output_vector_y/controller_output_vt\"): #will need to be changed when final output is coded\n",
    "                self.output_vector_weights = tf.get_variable(\"Wy\",shape=[self.controller_output_size,self.output_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "                self.controller_output_vector = tf.matmul(layer2_activation,self.output_vector_weights,name=\"controller_output_vector_vt\")\n",
    "                \n",
    "                \n",
    "                \n",
    "            with tf.name_scope(\"update_write_weightings/\"):\n",
    "                self.write_keys = tf.truncated_normal([1,word_size],name = \"temp_random_write_key\")\n",
    "                self.write_strength = self.one_plus(tf.truncated_normal([1,1],name = \"temp_random_write_strength\"),name=\"write_strengths_one_plus\")\n",
    "                self.erase_vector = tf.nn.sigmoid(tf.truncated_normal([1,word_size],name=\"temp_random_erase_vector\"),name=\"erase_vector_sigmoid\")\n",
    "                \n",
    "                self.allocation_gate = tf.nn.sigmoid(tf.truncated_normal([1,1],name=\"temp_random_allocation_gate\"),name=\"allocation_gate_sigmoid\")\n",
    "                self.write_gate = tf.nn.sigmoid(tf.truncated_normal([1,1],name=\"temp_random_write_gate\"),name=\"write_gate_sigmoid\")\n",
    "                \n",
    "                self.write_content_weighting = self.content_lookup(self.write_keys,self.write_strength,\"write\") \n",
    "                \n",
    "                \n",
    "                new_write_weightings = self.write_gate*((1-self.allocation_gate)*self.write_content_weighting)\n",
    "                self.write_weightings = tf.assign(self.write_weightings,new_write_weightings,name =\"update_write_weightings\")  \n",
    "                \n",
    "            with tf.name_scope(\"calc_write_vector/\"):\n",
    "                self.write_vector = tf.assign(self.write_vector,tf.truncated_normal([1,word_size],name=\"temp_write_vector\"))\n",
    "                \n",
    "                \n",
    "            with tf.name_scope(\"update_memory_matrix\"):\n",
    "                new_memory_matrix = self.memory_matrix*(1-tf.matmul(self.write_weightings,self.erase_vector)) + tf.matmul( self.write_weightings,self.write_vector)\n",
    "                self.memory_matrix = tf.assign(self.memory_matrix,new_memory_matrix,name=\"update_memory_matrix\")\n",
    "            \n",
    "            with tf.name_scope(\"update_read_weightingss/\"):\n",
    "                self.read_keys = tf.truncated_normal([read_heads,word_size],name=\"temp_random_read_keys\")\n",
    "                self.read_strengths = self.one_plus(tf.truncated_normal([read_heads,1],name = \"temp_random_read_strengths\"),name=\"read_strengths_one_plus\")\n",
    "                \n",
    "            \n",
    "            #TODO: multiply by read_mode\n",
    "                self.content_weigthing = self.content_lookup(self.read_keys,self.read_strengths,\"read\") #N*1\n",
    "                self.read_weightings = tf.assign(self.read_weightings, self.content_weigthing,name =\"update_read_weightings\")\n",
    "            \n",
    "            with tf.name_scope(\"calc_read_vectors/\"):\n",
    "                self.read_vectors = tf.assign(self.read_vectors, tf.transpose(tf.matmul(self.memory_matrix,self.read_weightings,transpose_a=True)),name  =\"update_read_vectors\")\n",
    "            \n",
    "            with tf.name_scope(\"calc_output_vector_y/weighted_read_vectors\"):\n",
    "                weighted_read_vectors = tf.matmul(tf.reshape(self.read_vectors,[1,self.read_heads*self.word_size]),self.read_vectors_weights,name = \"weighted_read_vectors\")\n",
    "                \n",
    "            \n",
    "            with tf.name_scope(\"calc_output_vector_y/\"):\n",
    "                self.output_vector = tf.add( self.controller_output_vector  , weighted_read_vectors , name=\"output_vector_y\")\n",
    "            \n",
    "            \n",
    "            with tf.name_scope(\"loss\"):    \n",
    "                self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.output_vector, labels=self.output_y))\n",
    "        \n",
    "            with tf.device('/device:CPU:0'):\n",
    "                tf.summary.scalar(\"cross_entropy_loss\",self.loss)\n",
    "                \n",
    "            with tf.name_scope(\"optimizer\"):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(self.loss)\n",
    " \n",
    "    def one_plus(self,x,name=\"\"):\n",
    "        return  tf.add( 1.0,tf.log(tf.expand_dims(tf.add(1.0,tf.exp(x)),0)),name=name)\n",
    "    \n",
    "    def content_lookup(self,key,key_strength,mode=\"write\"):\n",
    "        with tf.name_scope(mode+\"cosine_content_lookup\"):\n",
    "            normalized_memory = tf.nn.l2_normalize(self.memory_matrix,1) #N*W\n",
    "            normalized_key = tf.nn.l2_normalize(key,0) #1*W\n",
    "        \n",
    "        \n",
    "            z = tf.matmul(normalized_memory,normalized_key,transpose_a=False,transpose_b=True)\n",
    "            content_lookup_result = tf.reshape(tf.nn.softmax(z*tf.squeeze(key_strength),0),[self.num_words,self.read_heads])\n",
    "        \n",
    "        # N*1\n",
    "        return  content_lookup_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    #generate the input output sequences, randomly intialized\n",
    "    tf.reset_default_graph()\n",
    "    num_seq = 10\n",
    "    seq_len = 6\n",
    "    seq_width = 4\n",
    "    iterations = 600\n",
    "    con = np.random.randint(0, seq_width,size=seq_len)\n",
    "    seq = np.zeros((seq_len, seq_width))\n",
    "    seq[np.arange(seq_len), con] = 1\n",
    "    end = np.asarray([[-1]*seq_width])\n",
    "    zer = np.zeros((seq_len, seq_width))\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    debug_every = 1\n",
    "    final_i_data = np.concatenate((seq, zer), axis=0)\n",
    "    final_o_data = np.concatenate((zer, seq), axis=0)\n",
    "    with tf.Session() as session:\n",
    "        \n",
    "        dnc = DNC(input_size=seq_width, output_size=seq_width, seq_len=seq_len, num_words=20, word_size=5, read_heads=1)\n",
    "        log_writer.add_graph(session.graph)\n",
    "        \n",
    "        session.run(tf.global_variables_initializer())\n",
    "        feed_dict = {dnc.input_x : np.reshape(final_i_data[j],[1,seq_width]), dnc.output_y: np.reshape(final_o_data[j],[1,seq_width])}\n",
    "        \n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        rk,mm,cw = session.run([dnc.read_keys,dnc.memory_matrix,dnc.content_weigthing],feed_dict=feed_dict)\n",
    "        print(rk,mm,cw)\n",
    "        #print(start_mem)\n",
    "        ol,l,op,summ,mm = session.run([dnc.output_vector,dnc.loss,dnc.optimizer,merged_summary,dnc.read_keys], feed_dict=feed_dict)\n",
    "        \n",
    "        log_writer.add_summary(summ)\n",
    "        #print(l)\n",
    "        #print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39497289 -0.82659078  1.77593958 -1.23148978  0.59110743]] [[-1.48690271 -0.24559009  1.32002068 -0.61622483  0.19444641]\n",
      " [-0.19670375 -0.96104103 -0.07071274 -0.87835163 -1.9464916 ]\n",
      " [-0.33569089  0.05019461 -0.60443139  1.05275834  0.22924083]\n",
      " [ 1.43154204  1.70968616  0.74253392 -0.53904504  0.09785389]\n",
      " [-0.03216974  0.018766    1.48649657 -1.15948033 -0.89035749]\n",
      " [-0.63967073 -1.61195278 -0.427618    0.59085381 -0.18877554]\n",
      " [ 0.9726395   1.60470283  0.86295801  0.42248476  1.20804763]\n",
      " [ 0.20940953 -0.24888763 -0.1277148  -0.50363731 -1.34205472]\n",
      " [-0.08086148 -0.42332864  0.46347427  1.59289575 -0.83522516]\n",
      " [ 0.05782478 -0.15293057 -1.05900228 -0.85203379  1.88221967]\n",
      " [ 0.67070061 -0.86690456 -1.2280581  -0.10816561  1.04694688]\n",
      " [-1.36626101 -0.53223974  0.1692007   0.21239825 -1.06225777]\n",
      " [-1.03696978  0.24237126 -1.21351171 -0.58721548  0.3362208 ]\n",
      " [-0.9008314   1.20902419 -0.53596032 -0.79741406  0.82041115]\n",
      " [-0.43407959  0.65325016  1.04456627 -0.04720911 -1.14066851]\n",
      " [ 1.7450614  -0.49371591  0.14319338 -0.28969565 -1.59948981]\n",
      " [-1.45181894 -0.56909376 -1.20518732  0.02433681 -0.32523835]\n",
      " [-0.99889737  0.13584352  0.78662312  0.15822636 -0.41898999]\n",
      " [ 0.1733083   0.6115126  -0.73875844 -0.19871521 -0.18130565]\n",
      " [-0.30213559 -0.33260906 -0.5326159   0.70360202  1.85971928]] [[ 0.07759305]\n",
      " [ 0.03870972]\n",
      " [ 0.00862187]\n",
      " [ 0.08079659]\n",
      " [ 0.12441261]\n",
      " [ 0.04036476]\n",
      " [ 0.0772081 ]\n",
      " [ 0.03103413]\n",
      " [ 0.01693857]\n",
      " [ 0.12335175]\n",
      " [ 0.11472107]\n",
      " [ 0.01320162]\n",
      " [ 0.01603757]\n",
      " [ 0.02507465]\n",
      " [ 0.02140583]\n",
      " [ 0.07918421]\n",
      " [ 0.01089229]\n",
      " [ 0.02070051]\n",
      " [ 0.01188892]\n",
      " [ 0.06786222]]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "    with tf.device('/device:GPU:1'):\n",
    "        var = tf.Variable(tf.zeros([2,1]),trainable = False)\n",
    "    \n",
    "        var2 = tf.Variable(tf.ones([2,1]),trainable=False)\n",
    "        \n",
    "        var = tf.assign(var,tf.add(var,var2))\n",
    "        \n",
    "        return var,var2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as session:\n",
    "        var,var2 = create_graph()\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(5):\n",
    "            result = session.run(var)\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [ 1.]]\n",
      "[[ 2.]\n",
      " [ 2.]]\n",
      "[[ 3.]\n",
      " [ 3.]]\n",
      "[[ 4.]\n",
      " [ 4.]]\n",
      "[[ 5.]\n",
      " [ 5.]]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "one_plus() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-857e1e65b534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-857e1e65b534>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mmemory_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey_strength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey_stren_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-857e1e65b534>\u001b[0m in \u001b[0;36mcreate_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#key_strength = one_plus( x=tf.truncated_normal([1,1],name = \"temp_random_read_strengths\"),name=\"op\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mkey_strength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"temp_random_read_strengths\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mkey_strength_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_plus\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mkey_strength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"temp_random_read_strengths\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: one_plus() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "def content_lookup(memory_matrix,key,key_strength):\n",
    "        with tf.name_scope(\"cosine_content_lookup\"):\n",
    "            normalized_memory = tf.nn.l2_normalize(memory_matrix,1) #N*W\n",
    "            normalized_key = tf.nn.l2_normalize(key,0) #1*W\n",
    "        \n",
    "        \n",
    "            z = tf.matmul(normalized_memory,normalized_key,transpose_a=False,transpose_b=True)\n",
    "            content_lookup_result = tf.reshape(tf.nn.softmax(z*tf.squeeze(key_strength),0),[20,1])\n",
    "            #content_lookup_result = tf.nn.softmax(tf.losses.cosine_distance(normalized_memory, tf.nn.l2_normalize(key, 1), dim=0))\n",
    "            #content_lookup_result = normalized_memory\n",
    "        # N*1\n",
    "        return  content_lookup_result\n",
    "\n",
    "def one_plus(self,x,name=\"\"):\n",
    "        return  tf.add( 1.0,tf.log(tf.expand_dims(tf.add(1.0,tf.exp(x)),0)),name=name)\n",
    "\n",
    "def create_graph():\n",
    "    memory_matrix = tf.truncated_normal([20,5])\n",
    "    key  = tf.truncated_normal([1,5],name=\"temp_random_read_keys\")\n",
    "    #key_strength = one_plus( x=tf.truncated_normal([1,1],name = \"temp_random_read_strengths\"),name=\"op\")\n",
    "    key_strength = tf.truncated_normal([1,1],name = \"temp_random_read_strengths\")\n",
    "    key_strength_op = one_plus( key_strength,name = \"temp_random_read_strengths\")\n",
    "        \n",
    "        \n",
    "    result = content_lookup(memory_matrix,key,key_strength_op)\n",
    "    \n",
    "    return memory_matrix,key,key_strength,key_strength_op,result\n",
    "\n",
    "def train():    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as session:\n",
    "        memory_matrix,key,key_strength,key_stren_op,result = create_graph()\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "        mm,k,ks,ks_op,r = session.run([memory_matrix,key,key_strength,key_stren_op,result])\n",
    "        print(\"mem\",mm)\n",
    "        print(\"key\",k)\n",
    "        print(\"key stren\",ks)\n",
    "        print(\"key stren op\",ks_op)\n",
    "        print(\"resul\",np.sum(r))\n",
    "        print(\"----\")\n",
    "        print(1 + np.log(1+np.exp(ks)))\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1 + np.log(1+np.exp(-1.06015229))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
