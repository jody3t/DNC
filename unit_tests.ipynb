{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOGDIR = \"unit_logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_writer = tf.summary.FileWriter(LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNC:\n",
    "    def __init__(self,input_size,output_size,seq_len,num_words,word_size,read_heads,device = '/device:GPU:1'):\n",
    "        self.output_size = output_size\n",
    "        self.controller_output_size = output_size\n",
    "        self.read_heads = read_heads\n",
    "        self.word_size = word_size\n",
    "        self.num_words = num_words\n",
    "        \n",
    "        with tf.device(device):\n",
    "            #controller\n",
    "            \n",
    "            self.read_weightings = tf.Variable( tf.fill([num_words,read_heads],1e-8,name = \"read_weights_wr_init\"),trainable=False,name=\"read_weights_wr\")\n",
    "            self.read_vectors = tf.Variable(tf.fill([read_heads,word_size],1e-8,name=\"init_read_vectors\"),trainable=False,name=\"read_vectors_r\")\n",
    "            \n",
    "            self.memory_matrix = tf.Variable( tf.truncated_normal([num_words,word_size]),trainable=False,name = \"memory_matrix_M\")\n",
    "            \n",
    "            with tf.name_scope(\"calc_output_vector_y/\"):\n",
    "                self.read_vectors_weights = tf.get_variable(\"Wr\",shape=[self.read_heads*self.word_size,self.output_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            with tf.name_scope(\"train_data/inputs_x\"):        \n",
    "                \n",
    "                \n",
    "                self.input_x = tf.placeholder(tf.float32,shape=[1,input_size],name = \"input_x\")\n",
    "                \n",
    "            with tf.name_scope(\"train_data/outputs_y\"):    \n",
    "                self.output_y = tf.placeholder(tf.float32,shape=[1,output_size],name=\"output_y\")\n",
    "            \n",
    "            \n",
    "\n",
    "            with tf.variable_scope(\"feed_forward/layer1\"):\n",
    "                self.weights1 = tf.get_variable(\"weights1\",shape=[input_size,32],initializer=tf.contrib.layers.xavier_initializer())\n",
    "                self.biases1 = tf.get_variable(\"biases1\",shape=[32],initializer=tf.zeros_initializer())\n",
    "                layer1_activation = tf.nn.relu(tf.matmul(self.input_x,self.weights1)+self.biases1)\n",
    "            with tf.variable_scope(\"feed_forward/layer2\"):\n",
    "                self.weights2 = tf.get_variable(\"weights2\",shape=[32,output_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "                self.biases2 = tf.get_variable(\"biases2\",shape=[output_size])\n",
    "                layer2_activation = tf.nn.relu(tf.matmul(layer1_activation,self.weights2)+self.biases2)\n",
    "                self.output_logits = layer2_activation\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "            with tf.name_scope(\"calc_output_vector_y/controller_output_vt\"): #will need to be changed when final output is coded\n",
    "                self.output_vector_weights = tf.get_variable(\"Wy\",shape=[self.controller_output_size,self.output_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "                self.controller_output_vector = tf.matmul(layer2_activation,self.output_vector_weights,name=\"controller_output_vector_vt\")\n",
    "                \n",
    "                \n",
    "            \n",
    "            with tf.name_scope(\"update_read_weightingss/\"):\n",
    "                self.read_keys = tf.truncated_normal([read_heads,word_size],name=\"temp_random_read_keys\")\n",
    "                self.read_strengths = self.one_plus(tf.truncated_normal([read_heads,1],name = \"temp_random_read_strengths\"),name=\"read_strengths_one_plus\")\n",
    "            \n",
    "            #TODO: multiply by read_mode\n",
    "                self.content_weigthing = self.content_lookup(self.read_keys,self.read_strengths) #N*1\n",
    "                self.read_weightings = tf.assign(self.read_weightings, self.content_weigthing,name =\"update_read_weightings\")\n",
    "            \n",
    "            with tf.name_scope(\"calc_read_vectors/\"):\n",
    "                self.read_vectors = tf.assign(self.read_vectors, tf.transpose(tf.matmul(self.memory_matrix,self.read_weightings,transpose_a=True)),name  =\"update_read_vectors\")\n",
    "            \n",
    "            with tf.name_scope(\"calc_output_vector_y/weighted_read_vectors\"):\n",
    "                weighted_read_vectors = tf.matmul(tf.reshape(self.read_vectors,[1,self.read_heads*self.word_size]),self.read_vectors_weights,name = \"weighted_read_vectors\")\n",
    "                \n",
    "            \n",
    "            with tf.name_scope(\"calc_output_vector_y/\"):\n",
    "                self.output_vector = tf.add( self.controller_output_vector  , weighted_read_vectors , name=\"output_vector_y\")\n",
    "            \n",
    "            \n",
    "            with tf.name_scope(\"loss\"):    \n",
    "                self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.output_vector, labels=self.output_y))\n",
    "        \n",
    "            with tf.device('/device:CPU:0'):\n",
    "                tf.summary.scalar(\"cross_entropy_loss\",self.loss)\n",
    "                \n",
    "            with tf.name_scope(\"optimizer\"):\n",
    "                self.optimizer = tf.train.AdamOptimizer(learning_rate=0.00001).minimize(self.loss)\n",
    " \n",
    "    def one_plus(self,x,name=\"\"):\n",
    "        return  tf.add( 1.0,tf.log(tf.expand_dims(tf.add(1.0,tf.exp(x)),0)),name=name)\n",
    "    \n",
    "    def content_lookup(self,key,key_strength):\n",
    "        with tf.name_scope(\"cosine_content_lookup\"):\n",
    "            normalized_memory = tf.nn.l2_normalize(self.memory_matrix,1) #N*W\n",
    "            normalized_key = tf.nn.l2_normalize(key,0) #1*W\n",
    "        \n",
    "        \n",
    "            z = tf.matmul(normalized_memory,normalized_key,transpose_a=False,transpose_b=True)\n",
    "            content_lookup_result = tf.reshape(tf.nn.softmax(z*tf.squeeze(key_strength),0),[self.num_words,self.read_heads])\n",
    "        \n",
    "        # N*1\n",
    "        return  content_lookup_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    #generate the input output sequences, randomly intialized\n",
    "    tf.reset_default_graph()\n",
    "    num_seq = 10\n",
    "    seq_len = 6\n",
    "    seq_width = 4\n",
    "    iterations = 600\n",
    "    con = np.random.randint(0, seq_width,size=seq_len)\n",
    "    seq = np.zeros((seq_len, seq_width))\n",
    "    seq[np.arange(seq_len), con] = 1\n",
    "    end = np.asarray([[-1]*seq_width])\n",
    "    zer = np.zeros((seq_len, seq_width))\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    debug_every = 1\n",
    "    final_i_data = np.concatenate((seq, zer), axis=0)\n",
    "    final_o_data = np.concatenate((zer, seq), axis=0)\n",
    "    with tf.Session() as session:\n",
    "        \n",
    "        dnc = DNC(input_size=seq_width, output_size=seq_width, seq_len=seq_len, num_words=20, word_size=5, read_heads=1)\n",
    "        log_writer.add_graph(session.graph)\n",
    "        \n",
    "        session.run(tf.global_variables_initializer())\n",
    "        feed_dict = {dnc.input_x : np.reshape(final_i_data[j],[1,seq_width]), dnc.output_y: np.reshape(final_o_data[j],[1,seq_width])}\n",
    "        \n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        rk,mm,cw = session.run([dnc.read_keys,dnc.memory_matrix,dnc.content_weigthing],feed_dict=feed_dict)\n",
    "        print(rk,mm,cw)\n",
    "        #print(start_mem)\n",
    "        ol,l,op,summ,mm = session.run([dnc.output_vector,dnc.loss,dnc.optimizer,merged_summary,dnc.read_keys], feed_dict=feed_dict)\n",
    "        \n",
    "        log_writer.add_summary(summ)\n",
    "        #print(l)\n",
    "        #print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.59424162 -0.72244006  0.37448293  0.09541292  0.6779151 ]] [[  2.47669697e-01   1.16942859e+00  -7.63524890e-01   1.29428184e+00\n",
      "   -1.56634510e+00]\n",
      " [ -1.13119781e-01   1.05344331e+00   4.49118435e-01   7.14221716e-01\n",
      "    2.05895022e-01]\n",
      " [  4.29264382e-02  -1.29676044e+00  -1.28187943e+00  -1.20258379e+00\n",
      "   -1.08996356e+00]\n",
      " [  4.80944514e-01  -9.96630788e-01   4.22748715e-01  -1.65358746e+00\n",
      "   -1.13452148e+00]\n",
      " [  1.58233766e-03  -3.79273444e-01  -1.36837697e+00  -1.42595112e-01\n",
      "   -2.65732527e-01]\n",
      " [ -5.99190891e-01   8.83945227e-01   5.71001656e-02  -3.43652874e-01\n",
      "   -5.04611135e-01]\n",
      " [  9.59120333e-01  -1.72320569e+00   6.99574454e-03  -1.56161022e+00\n",
      "    8.28489602e-01]\n",
      " [  1.39877200e+00  -2.04522982e-01   1.97561696e-01   1.08912313e+00\n",
      "    4.40411836e-01]\n",
      " [ -5.43573976e-01   9.84363258e-01  -4.63951856e-01  -2.75325239e-01\n",
      "    3.20992410e-01]\n",
      " [  8.14128876e-01  -3.44241351e-01  -1.85590792e+00   1.18575120e+00\n",
      "    1.84317553e+00]\n",
      " [  2.62433261e-01   1.09336150e+00  -1.48179877e+00  -1.00544214e+00\n",
      "   -5.25933743e-01]\n",
      " [ -2.33718306e-01  -1.28296360e-01   7.19509721e-01  -1.03174174e+00\n",
      "   -9.87155616e-01]\n",
      " [ -2.27285609e-01  -9.28111494e-01  -1.67218950e-02  -1.39734709e+00\n",
      "   -1.63882434e-01]\n",
      " [  1.12268627e+00   1.07474454e-01  -7.09360898e-01   1.07643449e+00\n",
      "   -4.08821523e-01]\n",
      " [  7.89380446e-02   4.76841331e-01   4.07591075e-01  -1.39212441e+00\n",
      "   -4.72571552e-02]\n",
      " [ -1.99074733e+00   1.92840517e-01   1.86176872e+00  -6.62785232e-01\n",
      "    6.64480567e-01]\n",
      " [  2.68103153e-01  -1.59963381e+00  -5.69763005e-01  -1.17581522e+00\n",
      "   -4.18899246e-02]\n",
      " [  5.77820063e-01   1.14977384e+00   7.18842864e-01   1.02978849e+00\n",
      "   -6.77070618e-01]\n",
      " [  1.76703346e+00   1.90301311e+00  -1.03263903e+00   3.91311437e-01\n",
      "   -1.44166219e+00]\n",
      " [ -5.83003610e-02   3.78862679e-01   2.15690359e-01  -3.35073024e-01\n",
      "   -3.72871876e-01]] [[ 0.00960035]\n",
      " [ 0.09018551]\n",
      " [ 0.01027152]\n",
      " [ 0.01343896]\n",
      " [ 0.01003989]\n",
      " [ 0.01172055]\n",
      " [ 0.05395832]\n",
      " [ 0.08641293]\n",
      " [ 0.01672096]\n",
      " [ 0.07871966]\n",
      " [ 0.00167544]\n",
      " [ 0.01947933]\n",
      " [ 0.03434354]\n",
      " [ 0.01520667]\n",
      " [ 0.00883918]\n",
      " [ 0.46518663]\n",
      " [ 0.03613512]\n",
      " [ 0.02926831]\n",
      " [ 0.00232134]\n",
      " [ 0.00647581]]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "    with tf.device('/device:GPU:1'):\n",
    "        var = tf.Variable(tf.zeros([2,1]),trainable = False)\n",
    "    \n",
    "        var2 = tf.Variable(tf.ones([2,1]),trainable=False)\n",
    "        \n",
    "        var = tf.assign(var,tf.add(var,var2))\n",
    "        \n",
    "        return var,var2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as session:\n",
    "        var,var2 = create_graph()\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(5):\n",
    "            result = session.run(var)\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [ 1.]]\n",
      "[[ 2.]\n",
      " [ 2.]]\n",
      "[[ 3.]\n",
      " [ 3.]]\n",
      "[[ 4.]\n",
      " [ 4.]]\n",
      "[[ 5.]\n",
      " [ 5.]]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "one_plus() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-857e1e65b534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-857e1e65b534>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mmemory_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey_strength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey_stren_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-857e1e65b534>\u001b[0m in \u001b[0;36mcreate_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#key_strength = one_plus( x=tf.truncated_normal([1,1],name = \"temp_random_read_strengths\"),name=\"op\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mkey_strength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"temp_random_read_strengths\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mkey_strength_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_plus\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mkey_strength\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"temp_random_read_strengths\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: one_plus() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "def content_lookup(memory_matrix,key,key_strength):\n",
    "        with tf.name_scope(\"cosine_content_lookup\"):\n",
    "            normalized_memory = tf.nn.l2_normalize(memory_matrix,1) #N*W\n",
    "            normalized_key = tf.nn.l2_normalize(key,0) #1*W\n",
    "        \n",
    "        \n",
    "            z = tf.matmul(normalized_memory,normalized_key,transpose_a=False,transpose_b=True)\n",
    "            content_lookup_result = tf.reshape(tf.nn.softmax(z*tf.squeeze(key_strength),0),[20,1])\n",
    "            #content_lookup_result = tf.nn.softmax(tf.losses.cosine_distance(normalized_memory, tf.nn.l2_normalize(key, 1), dim=0))\n",
    "            #content_lookup_result = normalized_memory\n",
    "        # N*1\n",
    "        return  content_lookup_result\n",
    "\n",
    "def one_plus(self,x,name=\"\"):\n",
    "        return  tf.add( 1.0,tf.log(tf.expand_dims(tf.add(1.0,tf.exp(x)),0)),name=name)\n",
    "\n",
    "def create_graph():\n",
    "    memory_matrix = tf.truncated_normal([20,5])\n",
    "    key  = tf.truncated_normal([1,5],name=\"temp_random_read_keys\")\n",
    "    #key_strength = one_plus( x=tf.truncated_normal([1,1],name = \"temp_random_read_strengths\"),name=\"op\")\n",
    "    key_strength = tf.truncated_normal([1,1],name = \"temp_random_read_strengths\")\n",
    "    key_strength_op = one_plus( key_strength,name = \"temp_random_read_strengths\")\n",
    "        \n",
    "        \n",
    "    result = content_lookup(memory_matrix,key,key_strength_op)\n",
    "    \n",
    "    return memory_matrix,key,key_strength,key_strength_op,result\n",
    "\n",
    "def train():    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as session:\n",
    "        memory_matrix,key,key_strength,key_stren_op,result = create_graph()\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        \n",
    "        mm,k,ks,ks_op,r = session.run([memory_matrix,key,key_strength,key_stren_op,result])\n",
    "        print(\"mem\",mm)\n",
    "        print(\"key\",k)\n",
    "        print(\"key stren\",ks)\n",
    "        print(\"key stren op\",ks_op)\n",
    "        print(\"resul\",np.sum(r))\n",
    "        print(\"----\")\n",
    "        print(1 + np.log(1+np.exp(ks)))\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + np.log(1+np.exp(-1.06015229))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
